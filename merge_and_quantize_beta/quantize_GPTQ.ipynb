{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7caff11c-e0a0-4961-ba1d-04896e7c4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f830b47-d210-4eb3-95de-783a8966d81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
      "Collecting auto-gptq\n",
      "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.26.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.29.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets (from auto-gptq)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece (from auto-gptq)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.24.1)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu118)\n",
      "Collecting safetensors (from auto-gptq)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting transformers>=4.31.0 (from auto-gptq)\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.5.0 (from auto-gptq)\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tqdm (from auto-gptq)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Collecting huggingface-hub (from accelerate>=0.26.0->auto-gptq)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.31.0->auto-gptq)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.31.0->auto-gptq)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pyarrow>=12.0.0 (from datasets->auto-gptq)\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->auto-gptq)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto-gptq)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->auto-gptq)\n",
      "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets->auto-gptq)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->auto-gptq)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets->auto-gptq)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->auto-gptq)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub (from accelerate>=0.26.0->auto-gptq)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.21.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets->auto-gptq)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->auto-gptq)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->auto-gptq)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Downloading accelerate-0.29.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gekko-1.1.0-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m139.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, pytz, xxhash, tzdata, tqdm, safetensors, rouge, regex, pyarrow-hotfix, pyarrow, multidict, gekko, fsspec, frozenlist, dill, async-timeout, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, accelerate, transformers, peft, datasets, auto-gptq\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-0.29.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 auto-gptq-0.7.1+cu118 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 gekko-1.1.0 huggingface-hub-0.22.2 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.1 peft-0.10.0 pyarrow-15.0.2 pyarrow-hotfix-0.6 pytz-2024.1 regex-2023.12.25 rouge-1.0.1 safetensors-0.4.2 sentencepiece-0.2.0 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.39.3 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install auto-gptq --no-build-isolation --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ # bcs cuda 11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "761f4637-7f41-40ae-9eed-58b09c68c0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4472c5b6cef9482399c58ed3988ee55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab109e655aa04982be1005fb65a4ae22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1690aa9f7cab4167be6aa0768c7c1ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac74b147d9a9450db810a2aa411057bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c942138d019e41489a1adc1bd5e4fdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d7b3a3200345fa869de053e869a014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f04967c09b4308bc2be514be4121db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0986dd8fce1412ebbf5340f061d1d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "2024-04-05 14:56:33 INFO [auto_gptq.modeling._base] Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-04-05 14:56:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-04-05 14:56:38 INFO [auto_gptq.quantization.gptq] duration: 1.7417263984680176\n",
      "2024-04-05 14:56:38 INFO [auto_gptq.quantization.gptq] avg loss: 5.905703544616699\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-04-05 14:56:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-04-05 14:56:39 INFO [auto_gptq.quantization.gptq] duration: 1.265934705734253\n",
      "2024-04-05 14:56:39 INFO [auto_gptq.quantization.gptq] avg loss: 0.33390700817108154\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-04-05 14:56:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-04-05 14:56:41 INFO [auto_gptq.quantization.gptq] duration: 1.2588515281677246\n",
      "2024-04-05 14:56:41 INFO [auto_gptq.quantization.gptq] avg loss: 6.740706443786621\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-04-05 14:56:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-04-05 14:56:45 INFO [auto_gptq.quantization.gptq] duration: 1.6302311420440674\n",
      "2024-04-05 14:56:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.0019215631764382124\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-04-05 14:56:48 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-04-05 14:56:49 INFO [auto_gptq.quantization.gptq] duration: 1.6968035697937012\n",
      "2024-04-05 14:56:49 INFO [auto_gptq.quantization.gptq] avg loss: 1.4188024997711182\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-04-05 14:56:49 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-04-05 14:56:51 INFO [auto_gptq.quantization.gptq] duration: 1.2746691703796387\n",
      "2024-04-05 14:56:51 INFO [auto_gptq.quantization.gptq] avg loss: 1.4872090816497803\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-04-05 14:56:55 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-04-05 14:57:00 INFO [auto_gptq.quantization.gptq] duration: 4.42054557800293\n",
      "2024-04-05 14:57:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.01470239832997322\n",
      "INFO - Start quantizing layer 2/32\n",
      "2024-04-05 14:57:02 INFO [auto_gptq.modeling._base] Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-04-05 14:57:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-04-05 14:57:08 INFO [auto_gptq.quantization.gptq] duration: 1.667126178741455\n",
      "2024-04-05 14:57:08 INFO [auto_gptq.quantization.gptq] avg loss: 9.132440567016602\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-04-05 14:57:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-04-05 14:57:09 INFO [auto_gptq.quantization.gptq] duration: 1.238856554031372\n",
      "2024-04-05 14:57:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.9529752731323242\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-04-05 14:57:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-04-05 14:57:10 INFO [auto_gptq.quantization.gptq] duration: 1.2455415725708008\n",
      "2024-04-05 14:57:10 INFO [auto_gptq.quantization.gptq] avg loss: 8.955499649047852\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-04-05 14:57:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-04-05 14:57:14 INFO [auto_gptq.quantization.gptq] duration: 1.6800010204315186\n",
      "2024-04-05 14:57:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.04074542969465256\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-04-05 14:57:17 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-04-05 14:57:19 INFO [auto_gptq.quantization.gptq] duration: 1.7414796352386475\n",
      "2024-04-05 14:57:19 INFO [auto_gptq.quantization.gptq] avg loss: 4.9621429443359375\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-04-05 14:57:19 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-04-05 14:57:20 INFO [auto_gptq.quantization.gptq] duration: 1.2883844375610352\n",
      "2024-04-05 14:57:20 INFO [auto_gptq.quantization.gptq] avg loss: 5.685858249664307\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-04-05 14:57:25 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-04-05 14:57:29 INFO [auto_gptq.quantization.gptq] duration: 4.430680751800537\n",
      "2024-04-05 14:57:29 INFO [auto_gptq.quantization.gptq] avg loss: 15.975652694702148\n",
      "INFO - Start quantizing layer 3/32\n",
      "2024-04-05 14:57:32 INFO [auto_gptq.modeling._base] Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-04-05 14:57:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-04-05 14:57:37 INFO [auto_gptq.quantization.gptq] duration: 1.6591997146606445\n",
      "2024-04-05 14:57:37 INFO [auto_gptq.quantization.gptq] avg loss: 27.23836326599121\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-04-05 14:57:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-04-05 14:57:38 INFO [auto_gptq.quantization.gptq] duration: 1.2231769561767578\n",
      "2024-04-05 14:57:38 INFO [auto_gptq.quantization.gptq] avg loss: 6.31692361831665\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-04-05 14:57:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-04-05 14:57:39 INFO [auto_gptq.quantization.gptq] duration: 1.2246830463409424\n",
      "2024-04-05 14:57:39 INFO [auto_gptq.quantization.gptq] avg loss: 25.074724197387695\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-04-05 14:57:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-04-05 14:57:44 INFO [auto_gptq.quantization.gptq] duration: 1.6325600147247314\n",
      "2024-04-05 14:57:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.13107828795909882\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-04-05 14:57:47 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-04-05 14:57:48 INFO [auto_gptq.quantization.gptq] duration: 1.671018123626709\n",
      "2024-04-05 14:57:48 INFO [auto_gptq.quantization.gptq] avg loss: 12.477916717529297\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-04-05 14:57:48 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-04-05 14:57:49 INFO [auto_gptq.quantization.gptq] duration: 1.24479079246521\n",
      "2024-04-05 14:57:49 INFO [auto_gptq.quantization.gptq] avg loss: 14.415258407592773\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-04-05 14:57:54 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-04-05 14:57:58 INFO [auto_gptq.quantization.gptq] duration: 4.34686803817749\n",
      "2024-04-05 14:57:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.28200089931488037\n",
      "INFO - Start quantizing layer 4/32\n",
      "2024-04-05 14:58:01 INFO [auto_gptq.modeling._base] Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-04-05 14:58:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-04-05 14:58:06 INFO [auto_gptq.quantization.gptq] duration: 1.6618201732635498\n",
      "2024-04-05 14:58:06 INFO [auto_gptq.quantization.gptq] avg loss: 77.13632202148438\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-04-05 14:58:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-04-05 14:58:08 INFO [auto_gptq.quantization.gptq] duration: 1.2239069938659668\n",
      "2024-04-05 14:58:08 INFO [auto_gptq.quantization.gptq] avg loss: 19.471282958984375\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-04-05 14:58:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-04-05 14:58:09 INFO [auto_gptq.quantization.gptq] duration: 1.2319393157958984\n",
      "2024-04-05 14:58:09 INFO [auto_gptq.quantization.gptq] avg loss: 74.36770629882812\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-04-05 14:58:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-04-05 14:58:13 INFO [auto_gptq.quantization.gptq] duration: 1.6262173652648926\n",
      "2024-04-05 14:58:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.1449701189994812\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-04-05 14:58:16 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-04-05 14:58:17 INFO [auto_gptq.quantization.gptq] duration: 1.677720546722412\n",
      "2024-04-05 14:58:17 INFO [auto_gptq.quantization.gptq] avg loss: 25.964550018310547\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-04-05 14:58:17 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-04-05 14:58:19 INFO [auto_gptq.quantization.gptq] duration: 1.2422428131103516\n",
      "2024-04-05 14:58:19 INFO [auto_gptq.quantization.gptq] avg loss: 30.442394256591797\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-04-05 14:58:23 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-04-05 14:58:28 INFO [auto_gptq.quantization.gptq] duration: 4.394982814788818\n",
      "2024-04-05 14:58:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.5642273426055908\n",
      "INFO - Start quantizing layer 5/32\n",
      "2024-04-05 14:58:30 INFO [auto_gptq.modeling._base] Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-04-05 14:58:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-04-05 14:58:36 INFO [auto_gptq.quantization.gptq] duration: 1.6698338985443115\n",
      "2024-04-05 14:58:36 INFO [auto_gptq.quantization.gptq] avg loss: 78.64933776855469\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-04-05 14:58:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-04-05 14:58:37 INFO [auto_gptq.quantization.gptq] duration: 1.2373483180999756\n",
      "2024-04-05 14:58:37 INFO [auto_gptq.quantization.gptq] avg loss: 20.57801055908203\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-04-05 14:58:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-04-05 14:58:38 INFO [auto_gptq.quantization.gptq] duration: 1.235987663269043\n",
      "2024-04-05 14:58:38 INFO [auto_gptq.quantization.gptq] avg loss: 76.91966247558594\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-04-05 14:58:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-04-05 14:58:42 INFO [auto_gptq.quantization.gptq] duration: 1.6290512084960938\n",
      "2024-04-05 14:58:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.2382076382637024\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-04-05 14:58:45 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-04-05 14:58:47 INFO [auto_gptq.quantization.gptq] duration: 1.6816322803497314\n",
      "2024-04-05 14:58:47 INFO [auto_gptq.quantization.gptq] avg loss: 35.42518615722656\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-04-05 14:58:47 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-04-05 14:58:48 INFO [auto_gptq.quantization.gptq] duration: 1.2522683143615723\n",
      "2024-04-05 14:58:48 INFO [auto_gptq.quantization.gptq] avg loss: 43.97308349609375\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-04-05 14:58:53 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-04-05 14:58:57 INFO [auto_gptq.quantization.gptq] duration: 4.370098114013672\n",
      "2024-04-05 14:58:57 INFO [auto_gptq.quantization.gptq] avg loss: 1.0351406335830688\n",
      "INFO - Start quantizing layer 6/32\n",
      "2024-04-05 14:59:00 INFO [auto_gptq.modeling._base] Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-04-05 14:59:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-04-05 14:59:05 INFO [auto_gptq.quantization.gptq] duration: 1.673933982849121\n",
      "2024-04-05 14:59:05 INFO [auto_gptq.quantization.gptq] avg loss: 92.14930725097656\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-04-05 14:59:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-04-05 14:59:06 INFO [auto_gptq.quantization.gptq] duration: 1.2267775535583496\n",
      "2024-04-05 14:59:06 INFO [auto_gptq.quantization.gptq] avg loss: 24.512920379638672\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-04-05 14:59:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-04-05 14:59:07 INFO [auto_gptq.quantization.gptq] duration: 1.2263760566711426\n",
      "2024-04-05 14:59:07 INFO [auto_gptq.quantization.gptq] avg loss: 84.65599060058594\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-04-05 14:59:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-04-05 14:59:12 INFO [auto_gptq.quantization.gptq] duration: 1.6364057064056396\n",
      "2024-04-05 14:59:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.44331878423690796\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-04-05 14:59:14 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-04-05 14:59:16 INFO [auto_gptq.quantization.gptq] duration: 1.6820244789123535\n",
      "2024-04-05 14:59:16 INFO [auto_gptq.quantization.gptq] avg loss: 42.96813201904297\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-04-05 14:59:16 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-04-05 14:59:17 INFO [auto_gptq.quantization.gptq] duration: 1.2558774948120117\n",
      "2024-04-05 14:59:17 INFO [auto_gptq.quantization.gptq] avg loss: 53.723228454589844\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-04-05 14:59:22 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-04-05 14:59:26 INFO [auto_gptq.quantization.gptq] duration: 4.3746562004089355\n",
      "2024-04-05 14:59:26 INFO [auto_gptq.quantization.gptq] avg loss: 1.5521750450134277\n",
      "INFO - Start quantizing layer 7/32\n",
      "2024-04-05 14:59:29 INFO [auto_gptq.modeling._base] Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-04-05 14:59:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-04-05 14:59:34 INFO [auto_gptq.quantization.gptq] duration: 1.6968307495117188\n",
      "2024-04-05 14:59:34 INFO [auto_gptq.quantization.gptq] avg loss: 122.15348815917969\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-04-05 14:59:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-04-05 14:59:36 INFO [auto_gptq.quantization.gptq] duration: 1.2692031860351562\n",
      "2024-04-05 14:59:36 INFO [auto_gptq.quantization.gptq] avg loss: 34.165122985839844\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-04-05 14:59:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-04-05 14:59:37 INFO [auto_gptq.quantization.gptq] duration: 1.2491345405578613\n",
      "2024-04-05 14:59:37 INFO [auto_gptq.quantization.gptq] avg loss: 121.04840850830078\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-04-05 14:59:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-04-05 14:59:41 INFO [auto_gptq.quantization.gptq] duration: 1.6648015975952148\n",
      "2024-04-05 14:59:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.6804152727127075\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-04-05 14:59:44 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-04-05 14:59:46 INFO [auto_gptq.quantization.gptq] duration: 1.6855254173278809\n",
      "2024-04-05 14:59:46 INFO [auto_gptq.quantization.gptq] avg loss: 53.22158432006836\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-04-05 14:59:46 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-04-05 14:59:47 INFO [auto_gptq.quantization.gptq] duration: 1.2487196922302246\n",
      "2024-04-05 14:59:47 INFO [auto_gptq.quantization.gptq] avg loss: 69.18598937988281\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-04-05 14:59:52 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-04-05 14:59:56 INFO [auto_gptq.quantization.gptq] duration: 4.359065055847168\n",
      "2024-04-05 14:59:56 INFO [auto_gptq.quantization.gptq] avg loss: 2.294112205505371\n",
      "INFO - Start quantizing layer 8/32\n",
      "2024-04-05 14:59:59 INFO [auto_gptq.modeling._base] Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-04-05 15:00:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-04-05 15:00:04 INFO [auto_gptq.quantization.gptq] duration: 1.6745901107788086\n",
      "2024-04-05 15:00:04 INFO [auto_gptq.quantization.gptq] avg loss: 130.47901916503906\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-04-05 15:00:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-04-05 15:00:05 INFO [auto_gptq.quantization.gptq] duration: 1.225785255432129\n",
      "2024-04-05 15:00:05 INFO [auto_gptq.quantization.gptq] avg loss: 38.834617614746094\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-04-05 15:00:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-04-05 15:00:06 INFO [auto_gptq.quantization.gptq] duration: 1.2221250534057617\n",
      "2024-04-05 15:00:06 INFO [auto_gptq.quantization.gptq] avg loss: 131.25083923339844\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-04-05 15:00:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-04-05 15:00:10 INFO [auto_gptq.quantization.gptq] duration: 1.6306140422821045\n",
      "2024-04-05 15:00:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.8857259750366211\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-04-05 15:00:13 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-04-05 15:00:15 INFO [auto_gptq.quantization.gptq] duration: 1.6881310939788818\n",
      "2024-04-05 15:00:15 INFO [auto_gptq.quantization.gptq] avg loss: 62.703041076660156\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-04-05 15:00:15 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-04-05 15:00:16 INFO [auto_gptq.quantization.gptq] duration: 1.2562825679779053\n",
      "2024-04-05 15:00:16 INFO [auto_gptq.quantization.gptq] avg loss: 80.9369125366211\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-04-05 15:00:21 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-04-05 15:00:25 INFO [auto_gptq.quantization.gptq] duration: 4.36260724067688\n",
      "2024-04-05 15:00:25 INFO [auto_gptq.quantization.gptq] avg loss: 3.0732526779174805\n",
      "INFO - Start quantizing layer 9/32\n",
      "2024-04-05 15:00:28 INFO [auto_gptq.modeling._base] Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-04-05 15:00:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-04-05 15:00:33 INFO [auto_gptq.quantization.gptq] duration: 1.6775178909301758\n",
      "2024-04-05 15:00:33 INFO [auto_gptq.quantization.gptq] avg loss: 133.74365234375\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-04-05 15:00:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-04-05 15:00:35 INFO [auto_gptq.quantization.gptq] duration: 1.2440879344940186\n",
      "2024-04-05 15:00:35 INFO [auto_gptq.quantization.gptq] avg loss: 40.73971939086914\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-04-05 15:00:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-04-05 15:00:36 INFO [auto_gptq.quantization.gptq] duration: 1.2639133930206299\n",
      "2024-04-05 15:00:36 INFO [auto_gptq.quantization.gptq] avg loss: 132.82623291015625\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-04-05 15:00:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-04-05 15:00:40 INFO [auto_gptq.quantization.gptq] duration: 1.6445958614349365\n",
      "2024-04-05 15:00:40 INFO [auto_gptq.quantization.gptq] avg loss: 1.4053311347961426\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-04-05 15:00:43 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-04-05 15:00:45 INFO [auto_gptq.quantization.gptq] duration: 1.6848676204681396\n",
      "2024-04-05 15:00:45 INFO [auto_gptq.quantization.gptq] avg loss: 69.4155044555664\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-04-05 15:00:45 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-04-05 15:00:46 INFO [auto_gptq.quantization.gptq] duration: 1.2604553699493408\n",
      "2024-04-05 15:00:46 INFO [auto_gptq.quantization.gptq] avg loss: 84.74867248535156\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-04-05 15:00:50 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-04-05 15:00:55 INFO [auto_gptq.quantization.gptq] duration: 4.462704420089722\n",
      "2024-04-05 15:00:55 INFO [auto_gptq.quantization.gptq] avg loss: 3.9076015949249268\n",
      "INFO - Start quantizing layer 10/32\n",
      "2024-04-05 15:00:58 INFO [auto_gptq.modeling._base] Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-04-05 15:01:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-04-05 15:01:03 INFO [auto_gptq.quantization.gptq] duration: 1.662278652191162\n",
      "2024-04-05 15:01:03 INFO [auto_gptq.quantization.gptq] avg loss: 147.45065307617188\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-04-05 15:01:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-04-05 15:01:04 INFO [auto_gptq.quantization.gptq] duration: 1.220041036605835\n",
      "2024-04-05 15:01:04 INFO [auto_gptq.quantization.gptq] avg loss: 44.752769470214844\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-04-05 15:01:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-04-05 15:01:05 INFO [auto_gptq.quantization.gptq] duration: 1.2248225212097168\n",
      "2024-04-05 15:01:05 INFO [auto_gptq.quantization.gptq] avg loss: 139.52377319335938\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-04-05 15:01:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-04-05 15:01:09 INFO [auto_gptq.quantization.gptq] duration: 1.6336658000946045\n",
      "2024-04-05 15:01:09 INFO [auto_gptq.quantization.gptq] avg loss: 1.9371232986450195\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-04-05 15:01:12 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-04-05 15:01:14 INFO [auto_gptq.quantization.gptq] duration: 1.6777489185333252\n",
      "2024-04-05 15:01:14 INFO [auto_gptq.quantization.gptq] avg loss: 76.69477081298828\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-04-05 15:01:14 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-04-05 15:01:15 INFO [auto_gptq.quantization.gptq] duration: 1.2458419799804688\n",
      "2024-04-05 15:01:15 INFO [auto_gptq.quantization.gptq] avg loss: 90.3908462524414\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-04-05 15:01:20 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-04-05 15:01:24 INFO [auto_gptq.quantization.gptq] duration: 4.347221374511719\n",
      "2024-04-05 15:01:24 INFO [auto_gptq.quantization.gptq] avg loss: 4.719329833984375\n",
      "INFO - Start quantizing layer 11/32\n",
      "2024-04-05 15:01:27 INFO [auto_gptq.modeling._base] Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-04-05 15:01:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-04-05 15:01:32 INFO [auto_gptq.quantization.gptq] duration: 1.661952018737793\n",
      "2024-04-05 15:01:32 INFO [auto_gptq.quantization.gptq] avg loss: 155.1690673828125\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-04-05 15:01:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-04-05 15:01:33 INFO [auto_gptq.quantization.gptq] duration: 1.219552993774414\n",
      "2024-04-05 15:01:33 INFO [auto_gptq.quantization.gptq] avg loss: 46.024471282958984\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-04-05 15:01:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-04-05 15:01:35 INFO [auto_gptq.quantization.gptq] duration: 1.2191526889801025\n",
      "2024-04-05 15:01:35 INFO [auto_gptq.quantization.gptq] avg loss: 143.79202270507812\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-04-05 15:01:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-04-05 15:01:39 INFO [auto_gptq.quantization.gptq] duration: 1.6204819679260254\n",
      "2024-04-05 15:01:39 INFO [auto_gptq.quantization.gptq] avg loss: 2.5845208168029785\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-04-05 15:01:42 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-04-05 15:01:43 INFO [auto_gptq.quantization.gptq] duration: 1.677513599395752\n",
      "2024-04-05 15:01:43 INFO [auto_gptq.quantization.gptq] avg loss: 82.05683898925781\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-04-05 15:01:43 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-04-05 15:01:45 INFO [auto_gptq.quantization.gptq] duration: 1.2452466487884521\n",
      "2024-04-05 15:01:45 INFO [auto_gptq.quantization.gptq] avg loss: 94.39810943603516\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-04-05 15:01:49 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-04-05 15:01:54 INFO [auto_gptq.quantization.gptq] duration: 4.34745192527771\n",
      "2024-04-05 15:01:54 INFO [auto_gptq.quantization.gptq] avg loss: 5.341752529144287\n",
      "INFO - Start quantizing layer 12/32\n",
      "2024-04-05 15:01:56 INFO [auto_gptq.modeling._base] Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-04-05 15:02:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-04-05 15:02:02 INFO [auto_gptq.quantization.gptq] duration: 1.6776504516601562\n",
      "2024-04-05 15:02:02 INFO [auto_gptq.quantization.gptq] avg loss: 160.52850341796875\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-04-05 15:02:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-04-05 15:02:03 INFO [auto_gptq.quantization.gptq] duration: 1.2217137813568115\n",
      "2024-04-05 15:02:03 INFO [auto_gptq.quantization.gptq] avg loss: 61.14500427246094\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-04-05 15:02:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-04-05 15:02:04 INFO [auto_gptq.quantization.gptq] duration: 1.234184741973877\n",
      "2024-04-05 15:02:04 INFO [auto_gptq.quantization.gptq] avg loss: 159.44818115234375\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-04-05 15:02:07 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-04-05 15:02:08 INFO [auto_gptq.quantization.gptq] duration: 1.6335160732269287\n",
      "2024-04-05 15:02:08 INFO [auto_gptq.quantization.gptq] avg loss: 2.398127555847168\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-04-05 15:02:11 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-04-05 15:02:13 INFO [auto_gptq.quantization.gptq] duration: 1.674771785736084\n",
      "2024-04-05 15:02:13 INFO [auto_gptq.quantization.gptq] avg loss: 91.25679016113281\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-04-05 15:02:13 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-04-05 15:02:14 INFO [auto_gptq.quantization.gptq] duration: 1.2525007724761963\n",
      "2024-04-05 15:02:14 INFO [auto_gptq.quantization.gptq] avg loss: 102.37844848632812\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-04-05 15:02:19 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-04-05 15:02:23 INFO [auto_gptq.quantization.gptq] duration: 4.436850309371948\n",
      "2024-04-05 15:02:23 INFO [auto_gptq.quantization.gptq] avg loss: 5.767098426818848\n",
      "INFO - Start quantizing layer 13/32\n",
      "2024-04-05 15:02:26 INFO [auto_gptq.modeling._base] Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-04-05 15:02:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-04-05 15:02:31 INFO [auto_gptq.quantization.gptq] duration: 1.6617658138275146\n",
      "2024-04-05 15:02:31 INFO [auto_gptq.quantization.gptq] avg loss: 181.0947265625\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-04-05 15:02:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-04-05 15:02:32 INFO [auto_gptq.quantization.gptq] duration: 1.2257020473480225\n",
      "2024-04-05 15:02:32 INFO [auto_gptq.quantization.gptq] avg loss: 60.891937255859375\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-04-05 15:02:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-04-05 15:02:34 INFO [auto_gptq.quantization.gptq] duration: 1.2283306121826172\n",
      "2024-04-05 15:02:34 INFO [auto_gptq.quantization.gptq] avg loss: 169.448974609375\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-04-05 15:02:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-04-05 15:02:38 INFO [auto_gptq.quantization.gptq] duration: 1.653695821762085\n",
      "2024-04-05 15:02:38 INFO [auto_gptq.quantization.gptq] avg loss: 2.890434503555298\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-04-05 15:02:41 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-04-05 15:02:42 INFO [auto_gptq.quantization.gptq] duration: 1.6807880401611328\n",
      "2024-04-05 15:02:42 INFO [auto_gptq.quantization.gptq] avg loss: 99.11831665039062\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-04-05 15:02:42 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-04-05 15:02:44 INFO [auto_gptq.quantization.gptq] duration: 1.245546817779541\n",
      "2024-04-05 15:02:44 INFO [auto_gptq.quantization.gptq] avg loss: 107.83650207519531\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-04-05 15:02:48 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-04-05 15:02:53 INFO [auto_gptq.quantization.gptq] duration: 4.354339361190796\n",
      "2024-04-05 15:02:53 INFO [auto_gptq.quantization.gptq] avg loss: 6.684662818908691\n",
      "INFO - Start quantizing layer 14/32\n",
      "2024-04-05 15:02:55 INFO [auto_gptq.modeling._base] Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-04-05 15:02:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-04-05 15:03:00 INFO [auto_gptq.quantization.gptq] duration: 1.6627702713012695\n",
      "2024-04-05 15:03:00 INFO [auto_gptq.quantization.gptq] avg loss: 179.36038208007812\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-04-05 15:03:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-04-05 15:03:02 INFO [auto_gptq.quantization.gptq] duration: 1.2229368686676025\n",
      "2024-04-05 15:03:02 INFO [auto_gptq.quantization.gptq] avg loss: 67.36695861816406\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-04-05 15:03:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-04-05 15:03:03 INFO [auto_gptq.quantization.gptq] duration: 1.2216665744781494\n",
      "2024-04-05 15:03:03 INFO [auto_gptq.quantization.gptq] avg loss: 172.75439453125\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-04-05 15:03:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-04-05 15:03:07 INFO [auto_gptq.quantization.gptq] duration: 1.6275441646575928\n",
      "2024-04-05 15:03:07 INFO [auto_gptq.quantization.gptq] avg loss: 3.217472553253174\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-04-05 15:03:10 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-04-05 15:03:12 INFO [auto_gptq.quantization.gptq] duration: 1.6765716075897217\n",
      "2024-04-05 15:03:12 INFO [auto_gptq.quantization.gptq] avg loss: 105.48301696777344\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-04-05 15:03:12 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-04-05 15:03:13 INFO [auto_gptq.quantization.gptq] duration: 1.2561945915222168\n",
      "2024-04-05 15:03:13 INFO [auto_gptq.quantization.gptq] avg loss: 111.84712219238281\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-04-05 15:03:18 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-04-05 15:03:22 INFO [auto_gptq.quantization.gptq] duration: 4.422769784927368\n",
      "2024-04-05 15:03:22 INFO [auto_gptq.quantization.gptq] avg loss: 7.984584331512451\n",
      "INFO - Start quantizing layer 15/32\n",
      "2024-04-05 15:03:25 INFO [auto_gptq.modeling._base] Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-04-05 15:03:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-04-05 15:03:30 INFO [auto_gptq.quantization.gptq] duration: 1.7272636890411377\n",
      "2024-04-05 15:03:30 INFO [auto_gptq.quantization.gptq] avg loss: 187.047607421875\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-04-05 15:03:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-04-05 15:03:31 INFO [auto_gptq.quantization.gptq] duration: 1.2672739028930664\n",
      "2024-04-05 15:03:31 INFO [auto_gptq.quantization.gptq] avg loss: 67.02462768554688\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-04-05 15:03:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-04-05 15:03:33 INFO [auto_gptq.quantization.gptq] duration: 1.2700867652893066\n",
      "2024-04-05 15:03:33 INFO [auto_gptq.quantization.gptq] avg loss: 175.2174072265625\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-04-05 15:03:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-04-05 15:03:37 INFO [auto_gptq.quantization.gptq] duration: 1.6280593872070312\n",
      "2024-04-05 15:03:37 INFO [auto_gptq.quantization.gptq] avg loss: 3.9255869388580322\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-04-05 15:03:40 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-04-05 15:03:41 INFO [auto_gptq.quantization.gptq] duration: 1.6897938251495361\n",
      "2024-04-05 15:03:41 INFO [auto_gptq.quantization.gptq] avg loss: 116.18384552001953\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-04-05 15:03:41 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-04-05 15:03:43 INFO [auto_gptq.quantization.gptq] duration: 1.2607181072235107\n",
      "2024-04-05 15:03:43 INFO [auto_gptq.quantization.gptq] avg loss: 122.36404418945312\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-04-05 15:03:47 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-04-05 15:03:52 INFO [auto_gptq.quantization.gptq] duration: 4.404847860336304\n",
      "2024-04-05 15:03:52 INFO [auto_gptq.quantization.gptq] avg loss: 9.298858642578125\n",
      "INFO - Start quantizing layer 16/32\n",
      "2024-04-05 15:03:54 INFO [auto_gptq.modeling._base] Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-04-05 15:03:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-04-05 15:04:00 INFO [auto_gptq.quantization.gptq] duration: 1.6652109622955322\n",
      "2024-04-05 15:04:00 INFO [auto_gptq.quantization.gptq] avg loss: 182.75302124023438\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-04-05 15:04:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-04-05 15:04:01 INFO [auto_gptq.quantization.gptq] duration: 1.2252790927886963\n",
      "2024-04-05 15:04:01 INFO [auto_gptq.quantization.gptq] avg loss: 70.99109649658203\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-04-05 15:04:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-04-05 15:04:02 INFO [auto_gptq.quantization.gptq] duration: 1.2203691005706787\n",
      "2024-04-05 15:04:02 INFO [auto_gptq.quantization.gptq] avg loss: 171.01187133789062\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-04-05 15:04:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-04-05 15:04:06 INFO [auto_gptq.quantization.gptq] duration: 1.6284644603729248\n",
      "2024-04-05 15:04:06 INFO [auto_gptq.quantization.gptq] avg loss: 4.176673412322998\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-04-05 15:04:09 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-04-05 15:04:11 INFO [auto_gptq.quantization.gptq] duration: 1.691612958908081\n",
      "2024-04-05 15:04:11 INFO [auto_gptq.quantization.gptq] avg loss: 125.19950866699219\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-04-05 15:04:11 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-04-05 15:04:12 INFO [auto_gptq.quantization.gptq] duration: 1.2701213359832764\n",
      "2024-04-05 15:04:12 INFO [auto_gptq.quantization.gptq] avg loss: 131.58590698242188\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-04-05 15:04:17 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-04-05 15:04:21 INFO [auto_gptq.quantization.gptq] duration: 4.35886549949646\n",
      "2024-04-05 15:04:21 INFO [auto_gptq.quantization.gptq] avg loss: 11.341919898986816\n",
      "INFO - Start quantizing layer 17/32\n",
      "2024-04-05 15:04:24 INFO [auto_gptq.modeling._base] Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-04-05 15:04:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-04-05 15:04:29 INFO [auto_gptq.quantization.gptq] duration: 1.672560453414917\n",
      "2024-04-05 15:04:29 INFO [auto_gptq.quantization.gptq] avg loss: 186.48690795898438\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-04-05 15:04:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-04-05 15:04:30 INFO [auto_gptq.quantization.gptq] duration: 1.2249159812927246\n",
      "2024-04-05 15:04:30 INFO [auto_gptq.quantization.gptq] avg loss: 79.93486785888672\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-04-05 15:04:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-04-05 15:04:31 INFO [auto_gptq.quantization.gptq] duration: 1.2306585311889648\n",
      "2024-04-05 15:04:31 INFO [auto_gptq.quantization.gptq] avg loss: 175.74403381347656\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-04-05 15:04:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-04-05 15:04:36 INFO [auto_gptq.quantization.gptq] duration: 1.6311883926391602\n",
      "2024-04-05 15:04:36 INFO [auto_gptq.quantization.gptq] avg loss: 5.047186851501465\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-04-05 15:04:38 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-04-05 15:04:40 INFO [auto_gptq.quantization.gptq] duration: 1.6886718273162842\n",
      "2024-04-05 15:04:40 INFO [auto_gptq.quantization.gptq] avg loss: 134.79661560058594\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-04-05 15:04:40 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-04-05 15:04:41 INFO [auto_gptq.quantization.gptq] duration: 1.2861199378967285\n",
      "2024-04-05 15:04:41 INFO [auto_gptq.quantization.gptq] avg loss: 143.70230102539062\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-04-05 15:04:46 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-04-05 15:04:51 INFO [auto_gptq.quantization.gptq] duration: 4.392349481582642\n",
      "2024-04-05 15:04:51 INFO [auto_gptq.quantization.gptq] avg loss: 14.051470756530762\n",
      "INFO - Start quantizing layer 18/32\n",
      "2024-04-05 15:04:53 INFO [auto_gptq.modeling._base] Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-04-05 15:04:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-04-05 15:04:58 INFO [auto_gptq.quantization.gptq] duration: 1.6724519729614258\n",
      "2024-04-05 15:04:58 INFO [auto_gptq.quantization.gptq] avg loss: 183.67398071289062\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-04-05 15:04:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-04-05 15:05:00 INFO [auto_gptq.quantization.gptq] duration: 1.2336735725402832\n",
      "2024-04-05 15:05:00 INFO [auto_gptq.quantization.gptq] avg loss: 80.84953308105469\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-04-05 15:05:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-04-05 15:05:01 INFO [auto_gptq.quantization.gptq] duration: 1.236335039138794\n",
      "2024-04-05 15:05:01 INFO [auto_gptq.quantization.gptq] avg loss: 173.57058715820312\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-04-05 15:05:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-04-05 15:05:05 INFO [auto_gptq.quantization.gptq] duration: 1.6510581970214844\n",
      "2024-04-05 15:05:05 INFO [auto_gptq.quantization.gptq] avg loss: 3.8903231620788574\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-04-05 15:05:08 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-04-05 15:05:10 INFO [auto_gptq.quantization.gptq] duration: 1.6958980560302734\n",
      "2024-04-05 15:05:10 INFO [auto_gptq.quantization.gptq] avg loss: 152.54684448242188\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-04-05 15:05:10 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-04-05 15:05:11 INFO [auto_gptq.quantization.gptq] duration: 1.2492668628692627\n",
      "2024-04-05 15:05:11 INFO [auto_gptq.quantization.gptq] avg loss: 167.0294189453125\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-04-05 15:05:16 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-04-05 15:05:20 INFO [auto_gptq.quantization.gptq] duration: 4.368604898452759\n",
      "2024-04-05 15:05:20 INFO [auto_gptq.quantization.gptq] avg loss: 15.729948043823242\n",
      "INFO - Start quantizing layer 19/32\n",
      "2024-04-05 15:05:23 INFO [auto_gptq.modeling._base] Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-04-05 15:05:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-04-05 15:05:28 INFO [auto_gptq.quantization.gptq] duration: 1.659724473953247\n",
      "2024-04-05 15:05:28 INFO [auto_gptq.quantization.gptq] avg loss: 198.6794891357422\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-04-05 15:05:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-04-05 15:05:29 INFO [auto_gptq.quantization.gptq] duration: 1.222100019454956\n",
      "2024-04-05 15:05:29 INFO [auto_gptq.quantization.gptq] avg loss: 100.5677719116211\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-04-05 15:05:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-04-05 15:05:30 INFO [auto_gptq.quantization.gptq] duration: 1.2180733680725098\n",
      "2024-04-05 15:05:30 INFO [auto_gptq.quantization.gptq] avg loss: 190.476806640625\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-04-05 15:05:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-04-05 15:05:34 INFO [auto_gptq.quantization.gptq] duration: 1.6239969730377197\n",
      "2024-04-05 15:05:34 INFO [auto_gptq.quantization.gptq] avg loss: 4.195923805236816\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-04-05 15:05:37 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-04-05 15:05:39 INFO [auto_gptq.quantization.gptq] duration: 1.6740455627441406\n",
      "2024-04-05 15:05:39 INFO [auto_gptq.quantization.gptq] avg loss: 170.80101013183594\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-04-05 15:05:39 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-04-05 15:05:40 INFO [auto_gptq.quantization.gptq] duration: 1.241877794265747\n",
      "2024-04-05 15:05:40 INFO [auto_gptq.quantization.gptq] avg loss: 191.52426147460938\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-04-05 15:05:45 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-04-05 15:05:49 INFO [auto_gptq.quantization.gptq] duration: 4.380087375640869\n",
      "2024-04-05 15:05:49 INFO [auto_gptq.quantization.gptq] avg loss: 19.044567108154297\n",
      "INFO - Start quantizing layer 20/32\n",
      "2024-04-05 15:05:52 INFO [auto_gptq.modeling._base] Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-04-05 15:05:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-04-05 15:05:57 INFO [auto_gptq.quantization.gptq] duration: 1.6604232788085938\n",
      "2024-04-05 15:05:57 INFO [auto_gptq.quantization.gptq] avg loss: 196.41488647460938\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-04-05 15:05:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-04-05 15:05:58 INFO [auto_gptq.quantization.gptq] duration: 1.2301967144012451\n",
      "2024-04-05 15:05:58 INFO [auto_gptq.quantization.gptq] avg loss: 102.2266845703125\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-04-05 15:05:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-04-05 15:06:00 INFO [auto_gptq.quantization.gptq] duration: 1.2179651260375977\n",
      "2024-04-05 15:06:00 INFO [auto_gptq.quantization.gptq] avg loss: 187.74154663085938\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-04-05 15:06:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-04-05 15:06:04 INFO [auto_gptq.quantization.gptq] duration: 1.619598150253296\n",
      "2024-04-05 15:06:04 INFO [auto_gptq.quantization.gptq] avg loss: 4.110625267028809\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-04-05 15:06:07 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-04-05 15:06:08 INFO [auto_gptq.quantization.gptq] duration: 1.6851155757904053\n",
      "2024-04-05 15:06:08 INFO [auto_gptq.quantization.gptq] avg loss: 185.55087280273438\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-04-05 15:06:08 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-04-05 15:06:10 INFO [auto_gptq.quantization.gptq] duration: 1.2518877983093262\n",
      "2024-04-05 15:06:10 INFO [auto_gptq.quantization.gptq] avg loss: 209.38076782226562\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-04-05 15:06:14 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-04-05 15:06:19 INFO [auto_gptq.quantization.gptq] duration: 4.384325981140137\n",
      "2024-04-05 15:06:19 INFO [auto_gptq.quantization.gptq] avg loss: 21.2847957611084\n",
      "INFO - Start quantizing layer 21/32\n",
      "2024-04-05 15:06:21 INFO [auto_gptq.modeling._base] Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-04-05 15:06:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-04-05 15:06:27 INFO [auto_gptq.quantization.gptq] duration: 1.6722495555877686\n",
      "2024-04-05 15:06:27 INFO [auto_gptq.quantization.gptq] avg loss: 203.857177734375\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-04-05 15:06:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-04-05 15:06:28 INFO [auto_gptq.quantization.gptq] duration: 1.2321562767028809\n",
      "2024-04-05 15:06:28 INFO [auto_gptq.quantization.gptq] avg loss: 107.58919525146484\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-04-05 15:06:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-04-05 15:06:29 INFO [auto_gptq.quantization.gptq] duration: 1.2224929332733154\n",
      "2024-04-05 15:06:29 INFO [auto_gptq.quantization.gptq] avg loss: 195.04891967773438\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-04-05 15:06:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-04-05 15:06:56 INFO [auto_gptq.quantization.gptq] duration: 1.654736042022705\n",
      "2024-04-05 15:06:56 INFO [auto_gptq.quantization.gptq] avg loss: 214.34432983398438\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-04-05 15:06:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-04-05 15:06:57 INFO [auto_gptq.quantization.gptq] duration: 1.2178306579589844\n",
      "2024-04-05 15:06:57 INFO [auto_gptq.quantization.gptq] avg loss: 126.51927185058594\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-04-05 15:06:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-04-05 15:06:58 INFO [auto_gptq.quantization.gptq] duration: 1.2189927101135254\n",
      "2024-04-05 15:06:58 INFO [auto_gptq.quantization.gptq] avg loss: 209.1870880126953\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-04-05 15:07:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-04-05 15:07:03 INFO [auto_gptq.quantization.gptq] duration: 1.6258563995361328\n",
      "2024-04-05 15:07:03 INFO [auto_gptq.quantization.gptq] avg loss: 4.5586256980896\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-04-05 15:07:05 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-04-05 15:07:07 INFO [auto_gptq.quantization.gptq] duration: 1.6745262145996094\n",
      "2024-04-05 15:07:07 INFO [auto_gptq.quantization.gptq] avg loss: 212.41244506835938\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-04-05 15:07:07 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-04-05 15:07:08 INFO [auto_gptq.quantization.gptq] duration: 1.2424256801605225\n",
      "2024-04-05 15:07:08 INFO [auto_gptq.quantization.gptq] avg loss: 246.61851501464844\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-04-05 15:07:13 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-04-05 15:07:17 INFO [auto_gptq.quantization.gptq] duration: 4.360142707824707\n",
      "2024-04-05 15:07:17 INFO [auto_gptq.quantization.gptq] avg loss: 27.50237274169922\n",
      "INFO - Start quantizing layer 23/32\n",
      "2024-04-05 15:07:20 INFO [auto_gptq.modeling._base] Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-04-05 15:07:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-04-05 15:07:25 INFO [auto_gptq.quantization.gptq] duration: 1.7033262252807617\n",
      "2024-04-05 15:07:25 INFO [auto_gptq.quantization.gptq] avg loss: 233.65206909179688\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-04-05 15:07:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-04-05 15:07:27 INFO [auto_gptq.quantization.gptq] duration: 1.2657432556152344\n",
      "2024-04-05 15:07:27 INFO [auto_gptq.quantization.gptq] avg loss: 133.40029907226562\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-04-05 15:07:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-04-05 15:07:28 INFO [auto_gptq.quantization.gptq] duration: 1.2456107139587402\n",
      "2024-04-05 15:07:28 INFO [auto_gptq.quantization.gptq] avg loss: 226.99679565429688\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-04-05 15:07:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-04-05 15:07:32 INFO [auto_gptq.quantization.gptq] duration: 1.6746442317962646\n",
      "2024-04-05 15:07:32 INFO [auto_gptq.quantization.gptq] avg loss: 6.509756088256836\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-04-05 15:07:35 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-04-05 15:07:37 INFO [auto_gptq.quantization.gptq] duration: 1.7242765426635742\n",
      "2024-04-05 15:07:37 INFO [auto_gptq.quantization.gptq] avg loss: 230.2514190673828\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-04-05 15:07:37 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-04-05 15:07:38 INFO [auto_gptq.quantization.gptq] duration: 1.2834210395812988\n",
      "2024-04-05 15:07:38 INFO [auto_gptq.quantization.gptq] avg loss: 270.77044677734375\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-04-05 15:07:43 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-04-05 15:07:47 INFO [auto_gptq.quantization.gptq] duration: 4.4261474609375\n",
      "2024-04-05 15:07:47 INFO [auto_gptq.quantization.gptq] avg loss: 32.01026153564453\n",
      "INFO - Start quantizing layer 24/32\n",
      "2024-04-05 15:07:50 INFO [auto_gptq.modeling._base] Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-04-05 15:07:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-04-05 15:07:55 INFO [auto_gptq.quantization.gptq] duration: 1.661987543106079\n",
      "2024-04-05 15:07:55 INFO [auto_gptq.quantization.gptq] avg loss: 258.93023681640625\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-04-05 15:07:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-04-05 15:07:56 INFO [auto_gptq.quantization.gptq] duration: 1.2227423191070557\n",
      "2024-04-05 15:07:56 INFO [auto_gptq.quantization.gptq] avg loss: 167.0913543701172\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-04-05 15:07:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-04-05 15:07:57 INFO [auto_gptq.quantization.gptq] duration: 1.2290232181549072\n",
      "2024-04-05 15:07:57 INFO [auto_gptq.quantization.gptq] avg loss: 255.72145080566406\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-04-05 15:08:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-04-05 15:08:02 INFO [auto_gptq.quantization.gptq] duration: 1.6303272247314453\n",
      "2024-04-05 15:08:02 INFO [auto_gptq.quantization.gptq] avg loss: 6.555202484130859\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-04-05 15:08:04 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-04-05 15:08:06 INFO [auto_gptq.quantization.gptq] duration: 1.6778771877288818\n",
      "2024-04-05 15:08:06 INFO [auto_gptq.quantization.gptq] avg loss: 253.77638244628906\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-04-05 15:08:06 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-04-05 15:08:07 INFO [auto_gptq.quantization.gptq] duration: 1.2472236156463623\n",
      "2024-04-05 15:08:07 INFO [auto_gptq.quantization.gptq] avg loss: 295.25244140625\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-04-05 15:08:12 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-04-05 15:08:16 INFO [auto_gptq.quantization.gptq] duration: 4.3612470626831055\n",
      "2024-04-05 15:08:16 INFO [auto_gptq.quantization.gptq] avg loss: 36.330867767333984\n",
      "INFO - Start quantizing layer 25/32\n",
      "2024-04-05 15:08:19 INFO [auto_gptq.modeling._base] Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-04-05 15:08:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-04-05 15:08:24 INFO [auto_gptq.quantization.gptq] duration: 1.6578056812286377\n",
      "2024-04-05 15:08:24 INFO [auto_gptq.quantization.gptq] avg loss: 248.91140747070312\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-04-05 15:08:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-04-05 15:08:26 INFO [auto_gptq.quantization.gptq] duration: 1.227630615234375\n",
      "2024-04-05 15:08:26 INFO [auto_gptq.quantization.gptq] avg loss: 163.8359375\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-04-05 15:08:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-04-05 15:08:27 INFO [auto_gptq.quantization.gptq] duration: 1.2260513305664062\n",
      "2024-04-05 15:08:27 INFO [auto_gptq.quantization.gptq] avg loss: 240.9093017578125\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-04-05 15:08:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-04-05 15:08:31 INFO [auto_gptq.quantization.gptq] duration: 1.6379151344299316\n",
      "2024-04-05 15:08:31 INFO [auto_gptq.quantization.gptq] avg loss: 7.128546714782715\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-04-05 15:08:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-04-05 15:08:36 INFO [auto_gptq.quantization.gptq] duration: 1.6765081882476807\n",
      "2024-04-05 15:08:36 INFO [auto_gptq.quantization.gptq] avg loss: 274.1422119140625\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-04-05 15:08:36 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-04-05 15:08:37 INFO [auto_gptq.quantization.gptq] duration: 1.2481083869934082\n",
      "2024-04-05 15:08:37 INFO [auto_gptq.quantization.gptq] avg loss: 318.9566650390625\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-04-05 15:08:41 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-04-05 15:08:46 INFO [auto_gptq.quantization.gptq] duration: 4.362470626831055\n",
      "2024-04-05 15:08:46 INFO [auto_gptq.quantization.gptq] avg loss: 39.44093322753906\n",
      "INFO - Start quantizing layer 26/32\n",
      "2024-04-05 15:08:48 INFO [auto_gptq.modeling._base] Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-04-05 15:08:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-04-05 15:08:54 INFO [auto_gptq.quantization.gptq] duration: 1.6627225875854492\n",
      "2024-04-05 15:08:54 INFO [auto_gptq.quantization.gptq] avg loss: 288.4685974121094\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-04-05 15:08:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-04-05 15:08:55 INFO [auto_gptq.quantization.gptq] duration: 1.2322466373443604\n",
      "2024-04-05 15:08:55 INFO [auto_gptq.quantization.gptq] avg loss: 206.76312255859375\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-04-05 15:08:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-04-05 15:08:56 INFO [auto_gptq.quantization.gptq] duration: 1.239351511001587\n",
      "2024-04-05 15:08:56 INFO [auto_gptq.quantization.gptq] avg loss: 287.68505859375\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-04-05 15:08:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-04-05 15:09:00 INFO [auto_gptq.quantization.gptq] duration: 1.6372835636138916\n",
      "2024-04-05 15:09:00 INFO [auto_gptq.quantization.gptq] avg loss: 6.827005386352539\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-04-05 15:09:03 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-04-05 15:09:05 INFO [auto_gptq.quantization.gptq] duration: 1.6807739734649658\n",
      "2024-04-05 15:09:05 INFO [auto_gptq.quantization.gptq] avg loss: 294.34637451171875\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-04-05 15:09:05 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-04-05 15:09:06 INFO [auto_gptq.quantization.gptq] duration: 1.2491133213043213\n",
      "2024-04-05 15:09:06 INFO [auto_gptq.quantization.gptq] avg loss: 341.13665771484375\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-04-05 15:09:11 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-04-05 15:09:15 INFO [auto_gptq.quantization.gptq] duration: 4.352594614028931\n",
      "2024-04-05 15:09:15 INFO [auto_gptq.quantization.gptq] avg loss: 44.99552536010742\n",
      "INFO - Start quantizing layer 27/32\n",
      "2024-04-05 15:09:18 INFO [auto_gptq.modeling._base] Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-04-05 15:09:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-04-05 15:09:23 INFO [auto_gptq.quantization.gptq] duration: 1.702049732208252\n",
      "2024-04-05 15:09:23 INFO [auto_gptq.quantization.gptq] avg loss: 274.7322998046875\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-04-05 15:09:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-04-05 15:09:24 INFO [auto_gptq.quantization.gptq] duration: 1.2632133960723877\n",
      "2024-04-05 15:09:24 INFO [auto_gptq.quantization.gptq] avg loss: 204.8870849609375\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-04-05 15:09:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-04-05 15:09:26 INFO [auto_gptq.quantization.gptq] duration: 1.2534573078155518\n",
      "2024-04-05 15:09:26 INFO [auto_gptq.quantization.gptq] avg loss: 270.5118408203125\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-04-05 15:09:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-04-05 15:09:30 INFO [auto_gptq.quantization.gptq] duration: 1.6515932083129883\n",
      "2024-04-05 15:09:30 INFO [auto_gptq.quantization.gptq] avg loss: 10.163049697875977\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-04-05 15:09:33 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-04-05 15:09:34 INFO [auto_gptq.quantization.gptq] duration: 1.6924233436584473\n",
      "2024-04-05 15:09:34 INFO [auto_gptq.quantization.gptq] avg loss: 319.9990234375\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-04-05 15:09:34 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-04-05 15:09:36 INFO [auto_gptq.quantization.gptq] duration: 1.2440667152404785\n",
      "2024-04-05 15:09:36 INFO [auto_gptq.quantization.gptq] avg loss: 369.528076171875\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-04-05 15:09:40 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-04-05 15:09:45 INFO [auto_gptq.quantization.gptq] duration: 4.34505295753479\n",
      "2024-04-05 15:09:45 INFO [auto_gptq.quantization.gptq] avg loss: 50.510902404785156\n",
      "INFO - Start quantizing layer 28/32\n",
      "2024-04-05 15:09:47 INFO [auto_gptq.modeling._base] Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-04-05 15:09:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-04-05 15:09:52 INFO [auto_gptq.quantization.gptq] duration: 1.656832218170166\n",
      "2024-04-05 15:09:53 INFO [auto_gptq.quantization.gptq] avg loss: 299.76104736328125\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-04-05 15:09:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-04-05 15:09:54 INFO [auto_gptq.quantization.gptq] duration: 1.2376229763031006\n",
      "2024-04-05 15:09:54 INFO [auto_gptq.quantization.gptq] avg loss: 214.90927124023438\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-04-05 15:09:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-04-05 15:09:55 INFO [auto_gptq.quantization.gptq] duration: 1.239326000213623\n",
      "2024-04-05 15:09:55 INFO [auto_gptq.quantization.gptq] avg loss: 302.9920654296875\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-04-05 15:09:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-04-05 15:09:59 INFO [auto_gptq.quantization.gptq] duration: 1.6337471008300781\n",
      "2024-04-05 15:09:59 INFO [auto_gptq.quantization.gptq] avg loss: 11.195138931274414\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-04-05 15:10:02 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-04-05 15:10:04 INFO [auto_gptq.quantization.gptq] duration: 1.6752488613128662\n",
      "2024-04-05 15:10:04 INFO [auto_gptq.quantization.gptq] avg loss: 349.115478515625\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-04-05 15:10:04 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-04-05 15:10:05 INFO [auto_gptq.quantization.gptq] duration: 1.245270013809204\n",
      "2024-04-05 15:10:05 INFO [auto_gptq.quantization.gptq] avg loss: 400.2041015625\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-04-05 15:10:10 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-04-05 15:10:14 INFO [auto_gptq.quantization.gptq] duration: 4.34508490562439\n",
      "2024-04-05 15:10:14 INFO [auto_gptq.quantization.gptq] avg loss: 59.20706558227539\n",
      "INFO - Start quantizing layer 29/32\n",
      "2024-04-05 15:10:17 INFO [auto_gptq.modeling._base] Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-04-05 15:10:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-04-05 15:10:22 INFO [auto_gptq.quantization.gptq] duration: 1.6629846096038818\n",
      "2024-04-05 15:10:22 INFO [auto_gptq.quantization.gptq] avg loss: 298.66754150390625\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-04-05 15:10:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-04-05 15:10:23 INFO [auto_gptq.quantization.gptq] duration: 1.239675760269165\n",
      "2024-04-05 15:10:23 INFO [auto_gptq.quantization.gptq] avg loss: 239.39389038085938\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-04-05 15:10:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-04-05 15:10:24 INFO [auto_gptq.quantization.gptq] duration: 1.2261042594909668\n",
      "2024-04-05 15:10:24 INFO [auto_gptq.quantization.gptq] avg loss: 304.1003112792969\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-04-05 15:10:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-04-05 15:10:28 INFO [auto_gptq.quantization.gptq] duration: 1.6282024383544922\n",
      "2024-04-05 15:10:28 INFO [auto_gptq.quantization.gptq] avg loss: 14.229331016540527\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-04-05 15:10:31 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-04-05 15:10:33 INFO [auto_gptq.quantization.gptq] duration: 1.6725263595581055\n",
      "2024-04-05 15:10:33 INFO [auto_gptq.quantization.gptq] avg loss: 370.7544860839844\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-04-05 15:10:33 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-04-05 15:10:34 INFO [auto_gptq.quantization.gptq] duration: 1.246305227279663\n",
      "2024-04-05 15:10:34 INFO [auto_gptq.quantization.gptq] avg loss: 412.57562255859375\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-04-05 15:10:39 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-04-05 15:10:43 INFO [auto_gptq.quantization.gptq] duration: 4.350835561752319\n",
      "2024-04-05 15:10:43 INFO [auto_gptq.quantization.gptq] avg loss: 70.34899139404297\n",
      "INFO - Start quantizing layer 30/32\n",
      "2024-04-05 15:10:46 INFO [auto_gptq.modeling._base] Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-04-05 15:10:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-04-05 15:10:51 INFO [auto_gptq.quantization.gptq] duration: 1.6989386081695557\n",
      "2024-04-05 15:10:51 INFO [auto_gptq.quantization.gptq] avg loss: 268.9980163574219\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-04-05 15:10:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-04-05 15:10:52 INFO [auto_gptq.quantization.gptq] duration: 1.2272968292236328\n",
      "2024-04-05 15:10:52 INFO [auto_gptq.quantization.gptq] avg loss: 225.34500122070312\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-04-05 15:10:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-04-05 15:10:54 INFO [auto_gptq.quantization.gptq] duration: 1.2282764911651611\n",
      "2024-04-05 15:10:54 INFO [auto_gptq.quantization.gptq] avg loss: 275.3724365234375\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-04-05 15:10:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-04-05 15:10:58 INFO [auto_gptq.quantization.gptq] duration: 1.6362972259521484\n",
      "2024-04-05 15:10:58 INFO [auto_gptq.quantization.gptq] avg loss: 14.480875015258789\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-04-05 15:11:01 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-04-05 15:11:02 INFO [auto_gptq.quantization.gptq] duration: 1.6865308284759521\n",
      "2024-04-05 15:11:02 INFO [auto_gptq.quantization.gptq] avg loss: 383.99578857421875\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-04-05 15:11:02 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-04-05 15:11:04 INFO [auto_gptq.quantization.gptq] duration: 1.249319076538086\n",
      "2024-04-05 15:11:04 INFO [auto_gptq.quantization.gptq] avg loss: 422.3295593261719\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-04-05 15:11:08 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-04-05 15:11:13 INFO [auto_gptq.quantization.gptq] duration: 4.377010822296143\n",
      "2024-04-05 15:11:13 INFO [auto_gptq.quantization.gptq] avg loss: 84.07518005371094\n",
      "INFO - Start quantizing layer 31/32\n",
      "2024-04-05 15:11:15 INFO [auto_gptq.modeling._base] Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-04-05 15:11:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-04-05 15:11:21 INFO [auto_gptq.quantization.gptq] duration: 1.6748061180114746\n",
      "2024-04-05 15:11:21 INFO [auto_gptq.quantization.gptq] avg loss: 277.064208984375\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-04-05 15:11:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-04-05 15:11:22 INFO [auto_gptq.quantization.gptq] duration: 1.2453594207763672\n",
      "2024-04-05 15:11:22 INFO [auto_gptq.quantization.gptq] avg loss: 242.1746826171875\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-04-05 15:11:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-04-05 15:11:23 INFO [auto_gptq.quantization.gptq] duration: 1.2282302379608154\n",
      "2024-04-05 15:11:23 INFO [auto_gptq.quantization.gptq] avg loss: 294.46612548828125\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-04-05 15:11:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-04-05 15:11:27 INFO [auto_gptq.quantization.gptq] duration: 1.6299023628234863\n",
      "2024-04-05 15:11:27 INFO [auto_gptq.quantization.gptq] avg loss: 15.55630111694336\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-04-05 15:11:30 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-04-05 15:11:32 INFO [auto_gptq.quantization.gptq] duration: 1.6793527603149414\n",
      "2024-04-05 15:11:32 INFO [auto_gptq.quantization.gptq] avg loss: 386.3507080078125\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-04-05 15:11:32 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-04-05 15:11:33 INFO [auto_gptq.quantization.gptq] duration: 1.259575366973877\n",
      "2024-04-05 15:11:33 INFO [auto_gptq.quantization.gptq] avg loss: 431.4983825683594\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-04-05 15:11:38 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-04-05 15:11:42 INFO [auto_gptq.quantization.gptq] duration: 4.390341281890869\n",
      "2024-04-05 15:11:42 INFO [auto_gptq.quantization.gptq] avg loss: 130.28591918945312\n",
      "INFO - Start quantizing layer 32/32\n",
      "2024-04-05 15:11:45 INFO [auto_gptq.modeling._base] Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-04-05 15:11:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-04-05 15:11:50 INFO [auto_gptq.quantization.gptq] duration: 1.6593327522277832\n",
      "2024-04-05 15:11:50 INFO [auto_gptq.quantization.gptq] avg loss: 203.48367309570312\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-04-05 15:11:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-04-05 15:11:51 INFO [auto_gptq.quantization.gptq] duration: 1.2276296615600586\n",
      "2024-04-05 15:11:51 INFO [auto_gptq.quantization.gptq] avg loss: 135.19908142089844\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-04-05 15:11:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-04-05 15:11:52 INFO [auto_gptq.quantization.gptq] duration: 1.2245738506317139\n",
      "2024-04-05 15:11:52 INFO [auto_gptq.quantization.gptq] avg loss: 193.95826721191406\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-04-05 15:11:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-04-05 15:11:57 INFO [auto_gptq.quantization.gptq] duration: 1.6377511024475098\n",
      "2024-04-05 15:11:57 INFO [auto_gptq.quantization.gptq] avg loss: 17.69571876525879\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-04-05 15:12:00 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-04-05 15:12:01 INFO [auto_gptq.quantization.gptq] duration: 1.6734662055969238\n",
      "2024-04-05 15:12:01 INFO [auto_gptq.quantization.gptq] avg loss: 330.2469482421875\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-04-05 15:12:01 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-04-05 15:12:02 INFO [auto_gptq.quantization.gptq] duration: 1.243546962738037\n",
      "2024-04-05 15:12:02 INFO [auto_gptq.quantization.gptq] avg loss: 370.48187255859375\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-04-05 15:12:07 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-04-05 15:12:11 INFO [auto_gptq.quantization.gptq] duration: 4.3455469608306885\n",
      "2024-04-05 15:12:11 INFO [auto_gptq.quantization.gptq] avg loss: 296.896728515625\n",
      "2024-04-05 15:12:14 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "2024-04-05 15:12:17 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.q_proj\n",
      "2024-04-05 15:12:22 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.k_proj\n",
      "2024-04-05 15:12:25 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.v_proj\n",
      "2024-04-05 15:12:28 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.o_proj\n",
      "2024-04-05 15:12:31 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.gate_proj\n",
      "2024-04-05 15:12:40 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.up_proj\n",
      "2024-04-05 15:12:47 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.down_proj\n",
      "2024-04-05 15:12:54 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.q_proj\n",
      "2024-04-05 15:12:58 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.k_proj\n",
      "2024-04-05 15:13:01 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.v_proj\n",
      "2024-04-05 15:13:04 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.o_proj\n",
      "2024-04-05 15:13:11 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.gate_proj\n",
      "2024-04-05 15:13:21 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.up_proj\n",
      "2024-04-05 15:13:28 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.down_proj\n",
      "2024-04-05 15:13:35 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.q_proj\n",
      "2024-04-05 15:13:38 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.k_proj\n",
      "2024-04-05 15:13:41 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.v_proj\n",
      "2024-04-05 15:13:44 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.o_proj\n",
      "2024-04-05 15:13:47 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.gate_proj\n",
      "2024-04-05 15:13:53 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.up_proj\n",
      "2024-04-05 15:14:00 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.down_proj\n",
      "2024-04-05 15:14:05 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.q_proj\n",
      "2024-04-05 15:14:09 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.k_proj\n",
      "2024-04-05 15:14:13 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.v_proj\n",
      "2024-04-05 15:14:17 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.o_proj\n",
      "2024-04-05 15:14:26 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.gate_proj\n",
      "2024-04-05 15:14:38 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.up_proj\n",
      "2024-04-05 15:14:45 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.down_proj\n",
      "2024-04-05 15:14:51 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.q_proj\n",
      "2024-04-05 15:14:54 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.k_proj\n",
      "2024-04-05 15:14:57 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.v_proj\n",
      "2024-04-05 15:15:00 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.o_proj\n",
      "2024-04-05 15:15:03 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.gate_proj\n",
      "2024-04-05 15:15:11 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.up_proj\n",
      "2024-04-05 15:15:17 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.down_proj\n",
      "2024-04-05 15:15:30 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.q_proj\n",
      "2024-04-05 15:15:37 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.k_proj\n",
      "2024-04-05 15:15:42 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.v_proj\n",
      "2024-04-05 15:15:45 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.o_proj\n",
      "2024-04-05 15:15:49 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.gate_proj\n",
      "2024-04-05 15:15:56 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.up_proj\n",
      "2024-04-05 15:16:04 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.down_proj\n",
      "2024-04-05 15:16:09 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.q_proj\n",
      "2024-04-05 15:16:12 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.k_proj\n",
      "2024-04-05 15:16:16 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.v_proj\n",
      "2024-04-05 15:16:19 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.o_proj\n",
      "2024-04-05 15:16:22 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.gate_proj\n",
      "2024-04-05 15:16:29 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.up_proj\n",
      "2024-04-05 15:16:36 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.down_proj\n",
      "2024-04-05 15:16:41 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.q_proj\n",
      "2024-04-05 15:16:46 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.k_proj\n",
      "2024-04-05 15:16:53 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.v_proj\n",
      "2024-04-05 15:16:57 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.o_proj\n",
      "2024-04-05 15:17:01 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.gate_proj\n",
      "2024-04-05 15:17:10 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.up_proj\n",
      "2024-04-05 15:17:17 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.down_proj\n",
      "2024-04-05 15:17:23 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.q_proj\n",
      "2024-04-05 15:17:26 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.k_proj\n",
      "2024-04-05 15:17:29 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.v_proj\n",
      "2024-04-05 15:17:32 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.o_proj\n",
      "2024-04-05 15:17:35 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.gate_proj\n",
      "2024-04-05 15:17:42 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.up_proj\n",
      "2024-04-05 15:17:49 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.down_proj\n",
      "2024-04-05 15:17:55 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.q_proj\n",
      "2024-04-05 15:17:58 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.k_proj\n",
      "2024-04-05 15:18:01 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.v_proj\n",
      "2024-04-05 15:18:04 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.o_proj\n",
      "2024-04-05 15:18:08 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.gate_proj\n",
      "2024-04-05 15:18:17 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.up_proj\n",
      "2024-04-05 15:18:29 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.down_proj\n",
      "2024-04-05 15:18:36 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.q_proj\n",
      "2024-04-05 15:18:40 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.k_proj\n",
      "2024-04-05 15:18:43 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.v_proj\n",
      "2024-04-05 15:18:46 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.o_proj\n",
      "2024-04-05 15:18:50 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.gate_proj\n",
      "2024-04-05 15:18:57 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.up_proj\n",
      "2024-04-05 15:19:04 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.down_proj\n",
      "2024-04-05 15:19:11 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.q_proj\n",
      "2024-04-05 15:19:14 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.k_proj\n",
      "2024-04-05 15:19:18 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.v_proj\n",
      "2024-04-05 15:19:21 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.o_proj\n",
      "2024-04-05 15:19:25 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.gate_proj\n",
      "2024-04-05 15:19:39 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.up_proj\n",
      "2024-04-05 15:19:51 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.down_proj\n",
      "2024-04-05 15:19:56 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.q_proj\n",
      "2024-04-05 15:19:59 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.k_proj\n",
      "2024-04-05 15:20:02 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.v_proj\n",
      "2024-04-05 15:20:06 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.o_proj\n",
      "2024-04-05 15:20:10 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.gate_proj\n",
      "2024-04-05 15:20:16 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.up_proj\n",
      "2024-04-05 15:20:23 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.down_proj\n",
      "2024-04-05 15:20:29 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.q_proj\n",
      "2024-04-05 15:20:32 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.k_proj\n",
      "2024-04-05 15:20:35 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.v_proj\n",
      "2024-04-05 15:20:38 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.o_proj\n",
      "2024-04-05 15:20:43 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.gate_proj\n",
      "2024-04-05 15:20:53 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.up_proj\n",
      "2024-04-05 15:21:06 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.down_proj\n",
      "2024-04-05 15:21:12 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.q_proj\n",
      "2024-04-05 15:21:15 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.k_proj\n",
      "2024-04-05 15:21:18 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.v_proj\n",
      "2024-04-05 15:21:21 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.o_proj\n",
      "2024-04-05 15:21:24 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.gate_proj\n",
      "2024-04-05 15:21:33 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.up_proj\n",
      "2024-04-05 15:21:40 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.down_proj\n",
      "2024-04-05 15:21:46 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.q_proj\n",
      "2024-04-05 15:21:50 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.k_proj\n",
      "2024-04-05 15:21:53 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.v_proj\n",
      "2024-04-05 15:21:56 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.o_proj\n",
      "2024-04-05 15:22:00 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.gate_proj\n",
      "2024-04-05 15:22:07 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.up_proj\n",
      "2024-04-05 15:22:13 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.down_proj\n",
      "2024-04-05 15:22:20 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.q_proj\n",
      "2024-04-05 15:22:25 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.k_proj\n",
      "2024-04-05 15:22:28 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.v_proj\n",
      "2024-04-05 15:22:32 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.o_proj\n",
      "2024-04-05 15:22:35 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.gate_proj\n",
      "2024-04-05 15:22:43 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.up_proj\n",
      "2024-04-05 15:22:55 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.down_proj\n",
      "2024-04-05 15:23:03 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.q_proj\n",
      "2024-04-05 15:23:06 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.k_proj\n",
      "2024-04-05 15:23:10 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.v_proj\n",
      "2024-04-05 15:23:13 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.o_proj\n",
      "2024-04-05 15:23:18 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.gate_proj\n",
      "2024-04-05 15:23:25 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.up_proj\n",
      "2024-04-05 15:23:33 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.down_proj\n",
      "2024-04-05 15:23:40 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.q_proj\n",
      "2024-04-05 15:23:44 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.k_proj\n",
      "2024-04-05 15:23:47 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.v_proj\n",
      "2024-04-05 15:23:50 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.o_proj\n",
      "2024-04-05 15:23:54 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.gate_proj\n",
      "2024-04-05 15:24:02 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.up_proj\n",
      "2024-04-05 15:24:09 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.down_proj\n",
      "2024-04-05 15:24:23 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.q_proj\n",
      "2024-04-05 15:24:31 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.k_proj\n",
      "2024-04-05 15:24:35 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.v_proj\n",
      "2024-04-05 15:24:39 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.o_proj\n",
      "2024-04-05 15:24:42 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.gate_proj\n",
      "2024-04-05 15:24:50 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.up_proj\n",
      "2024-04-05 15:24:58 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.down_proj\n",
      "2024-04-05 15:25:05 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.q_proj\n",
      "2024-04-05 15:25:08 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.k_proj\n",
      "2024-04-05 15:25:11 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.v_proj\n",
      "2024-04-05 15:25:15 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.o_proj\n",
      "2024-04-05 15:25:17 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.gate_proj\n",
      "2024-04-05 15:25:25 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.up_proj\n",
      "2024-04-05 15:25:32 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.down_proj\n",
      "2024-04-05 15:25:38 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.q_proj\n",
      "2024-04-05 15:25:42 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.k_proj\n",
      "2024-04-05 15:25:49 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.v_proj\n",
      "2024-04-05 15:25:53 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.o_proj\n",
      "2024-04-05 15:25:57 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.gate_proj\n",
      "2024-04-05 15:26:08 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.up_proj\n",
      "2024-04-05 15:26:17 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.down_proj\n",
      "2024-04-05 15:26:25 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.q_proj\n",
      "2024-04-05 15:26:28 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.k_proj\n",
      "2024-04-05 15:26:33 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.v_proj\n",
      "2024-04-05 15:26:38 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.o_proj\n",
      "2024-04-05 15:26:42 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.gate_proj\n",
      "2024-04-05 15:26:51 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.up_proj\n",
      "2024-04-05 15:26:57 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.down_proj\n",
      "2024-04-05 15:27:04 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.q_proj\n",
      "2024-04-05 15:27:07 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.k_proj\n",
      "2024-04-05 15:27:10 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.v_proj\n",
      "2024-04-05 15:27:15 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.o_proj\n",
      "2024-04-05 15:27:19 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.gate_proj\n",
      "2024-04-05 15:27:32 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.up_proj\n",
      "2024-04-05 15:27:40 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.down_proj\n",
      "2024-04-05 15:27:46 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.q_proj\n",
      "2024-04-05 15:27:50 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.k_proj\n",
      "2024-04-05 15:27:53 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.v_proj\n",
      "2024-04-05 15:27:56 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.o_proj\n",
      "2024-04-05 15:28:00 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.gate_proj\n",
      "2024-04-05 15:28:08 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.up_proj\n",
      "2024-04-05 15:28:14 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.down_proj\n",
      "2024-04-05 15:28:20 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.q_proj\n",
      "2024-04-05 15:28:23 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.k_proj\n",
      "2024-04-05 15:28:26 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.v_proj\n",
      "2024-04-05 15:28:29 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.o_proj\n",
      "2024-04-05 15:28:34 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.gate_proj\n",
      "2024-04-05 15:28:41 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.up_proj\n",
      "2024-04-05 15:28:49 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.down_proj\n",
      "2024-04-05 15:28:55 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.q_proj\n",
      "2024-04-05 15:29:00 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.k_proj\n",
      "2024-04-05 15:29:03 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.v_proj\n",
      "2024-04-05 15:29:08 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.o_proj\n",
      "2024-04-05 15:29:11 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.gate_proj\n",
      "2024-04-05 15:29:18 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.up_proj\n",
      "2024-04-05 15:29:24 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.down_proj\n",
      "2024-04-05 15:29:29 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.q_proj\n",
      "2024-04-05 15:29:33 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.k_proj\n",
      "2024-04-05 15:29:36 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.v_proj\n",
      "2024-04-05 15:29:40 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.o_proj\n",
      "2024-04-05 15:29:43 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.gate_proj\n",
      "2024-04-05 15:29:51 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.up_proj\n",
      "2024-04-05 15:29:59 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.down_proj\n",
      "2024-04-05 15:30:06 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.q_proj\n",
      "2024-04-05 15:30:09 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.k_proj\n",
      "2024-04-05 15:30:13 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.v_proj\n",
      "2024-04-05 15:30:16 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.o_proj\n",
      "2024-04-05 15:30:21 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.gate_proj\n",
      "2024-04-05 15:30:28 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.up_proj\n",
      "2024-04-05 15:30:37 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.down_proj\n",
      "2024-04-05 15:30:42 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.q_proj\n",
      "2024-04-05 15:30:44 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.k_proj\n",
      "2024-04-05 15:30:47 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.v_proj\n",
      "2024-04-05 15:30:50 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.o_proj\n",
      "2024-04-05 15:30:55 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.gate_proj\n",
      "2024-04-05 15:31:01 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.up_proj\n",
      "2024-04-05 15:31:07 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.down_proj\n",
      "2024-04-05 15:31:13 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.q_proj\n",
      "2024-04-05 15:31:18 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.k_proj\n",
      "2024-04-05 15:31:27 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.v_proj\n",
      "2024-04-05 15:31:41 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.o_proj\n",
      "2024-04-05 15:31:45 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.gate_proj\n",
      "2024-04-05 15:31:56 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.up_proj\n",
      "2024-04-05 15:32:03 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.down_proj\n",
      "2024-04-05 15:32:10 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.q_proj\n",
      "2024-04-05 15:32:13 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.k_proj\n",
      "2024-04-05 15:32:16 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.v_proj\n",
      "2024-04-05 15:32:21 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.o_proj\n",
      "2024-04-05 15:32:24 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.gate_proj\n",
      "2024-04-05 15:32:32 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.up_proj\n",
      "2024-04-05 15:32:39 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.down_proj\n",
      "2024-04-05 15:32:46 INFO [auto_gptq.modeling._utils] Model packed.\n"
     ]
    }
   ],
   "source": [
    "# based off the auto-gptq repo example\n",
    "\n",
    "pretrained_model_dir = \"\"\n",
    "quantized_model_dir = \"\"\n",
    "\n",
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "\n",
    "traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=False)\n",
    "except Exception:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
    "trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
    "traindataset = []\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "for _ in range(128):\n",
    "    i = random.randint(0, trainenc.input_ids.shape[1] - 2048 - 1)\n",
    "    j = i + 2048\n",
    "    inp = trainenc.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(inp)\n",
    "    traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")\n",
    "\n",
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
    "\n",
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "model.quantize(traindataset)\n",
    "\n",
    "# save quantized model using safetensors\n",
    "model.save_quantized(quantized_model_dir, use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450a5484-38e1-492b-881b-87835e1e043f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_15_20mar_4bit/tokenizer_config.json',\n",
       " 'merged_15_20mar_4bit/special_tokens_map.json',\n",
       " 'merged_15_20mar_4bit/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9c453-23b6-408d-9b3b-63f0bee984cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ba98362b3b4d91aff649839aa70ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gptq_model-4bit-128g.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=\"\",\n",
    "    folder_path=quantized_model_dir,\n",
    "    token = \"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370b0814-8380-48ff-a13a-eba00b8ea4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 18 23:44:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:A1:00.0 Off |                  N/A |\n",
      "| 30%   38C    P8              32W / 350W |  14580MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2cb18-0319-456a-a38d-72fd08be6223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
